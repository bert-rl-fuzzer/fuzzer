from dataclasses import dataclass
from time import time

import argparse
import numpy as np
import pandas as pd
import tensorflow as tf
import wandb
from tensorflow import keras
from tqdm import tqdm
import logging
import os
import random

from bert import BERTActorCriticModel
from db import define_expt_tag as define_expt_tag_db
from bert import define_expt_tag as define_expt_tag_bert
from fuzz_env import RLFuzzEnv, NaiveFuzzEnv
from gru import GRUActorCriticModel
from ppo import Buffer, PPO
from preprocess import SQLPreprocess

pd.set_option('display.max_rows', 500)
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

parser = argparse.ArgumentParser()
parser.add_argument('--expt_tag', type=str, required=True)
parser.add_argument('--model_variant', choices=['BERT', 'GRU', 'RandomFuzzer', 'GrammarGenRandMutate', 'GrammarGenAndMutate'], type=str, required=True)
parser.add_argument('--logging', choices=['debug', 'info', 'normal'], type=str, required=True)
args = parser.parse_args()

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

COLAB_FLAG = False  # True when running on Google Colab
CREATE_DB = True # to prevent clashing multiple db creations
retrain_bert = False # set it to True to create a fresh BERT MLM
EXPT_TAG = args.expt_tag # "mutation_logs"
model_variant = args.model_variant # Choose from BERT, GRU

# Priority order: DEBUG -> INFO -> WARNING -> ERROR -> CRITICAL
if args.logging == 'debug':
    logging.basicConfig(level=logging.DEBUG)
elif args.logging == 'info':
    logging.basicConfig(level=logging.INFO)
elif args.logging == 'normal':
    logging.basicConfig(level=logging.WARNING)
else:
    raise ValueError(f"Invalid choice of logging mode: {args.logging}")

run = wandb.init(reinit=True, tags=["initial-setup", EXPT_TAG], notes=EXPT_TAG, project="attention-fuzzer", settings=wandb.Settings(start_method='fork'))

print("Experiment tag: ", EXPT_TAG)
define_expt_tag_db(EXPT_TAG)
define_expt_tag_bert(EXPT_TAG)

# Hyperparameters of the PPO algorithm

MODEL_NAME = "AttentiveFuzzer"
steps_per_epoch = 5
epochs = 10
gamma = 0.99
clip_ratio = 0.2
policy_learning_rate = 3e-4
value_function_learning_rate = 1e-3
train_policy_iterations = 80
train_value_iterations = 80
lam = 0.97
target_kl = 0.01
hidden_sizes = (128, 64)

preferred_col_name = ['email', 'pass'] # copied after fetching from crawler

MAX_LENGTH = 11  # including EOS

# Cleanup esp to generalize column names and table names

str_replace_dict = {
    'DUAL': 'table_name',
    'users': 'table_name',
    'user': 'table_name',
    'all_tables': 'table_name',
    'information_schema.tables': 'table_name',
    'v$version': 'table_name',
    'v$instance': 'table_name',
    'email': 'col_name',
    'password': 'col_name',
    'pass': 'col_name',
    'banner': 'col_name',
    'version': 'col_name',
    '[': '(',
    ']': ')'
}

# manually selecting the bi-grams

bigrams_processed_list = [('union', 'select'),
                          ('from', 'table_name'),
                          ('select', 'null'),
                          (',', 'col_name'),
                          ('(', "'"),
                          ('null', ','),
                          ("'", 'union'),
                          ('col_name', ','),
                          ('/', '*'),
                          ("'", ')'),
                          ('order', 'by'),
                          ('table_name', ';'),
                          ('col_name', 'from'),
                          ('*', ')'),
                          ('#', ')'),
                          ('--', ')'),
                          ('table_name', '/'),
                          ('table_name', '#'),
                          ('table_name', '--'),
                          (';', '#'),
                          (';', '/'),
                          (';', '--'),
                          ("'", 'or'),
                          ('select', '*'),
                          ('*', 'from'),
                          ('select', 'col_name')]

# all_grammar_inputs stores all the strings generated by a grammar-based generator

# if COLAB_FLAG:
#     import sys
#     from google.colab import drive
#
#     drive.mount('/content/gdrive/')
#     sys.path.append('/content/gdrive/My Drive/python')
#
# if COLAB_FLAG:
#     with open("/content/gdrive/My Drive/python/grammar_lib/all_grammar_inputs.txt") as file:
#         all_grammar_inputs = [line.strip() for line in file]
# else:
#     with open("grammar_lib/all_grammar_inputs.txt") as file:
#         all_grammar_inputs = [line.strip() for line in file]

# all_grammar_inputs stores all the strings generated by a grammar-based generator

# with open("AttentionFuzzer/attention_fuzzer/grammar_lib/all_grammar_inputs.txt") as file:
#     all_grammar_inputs = [line.strip() for line in file]

sql_preprocess = SQLPreprocess(str_replace_dict, bigrams_processed_list)

all_grammar_inputs = sql_preprocess.hand_pick_sql_queries() # handpick sql queries

all_grammar_inputs = sql_preprocess.generalize_tokens(all_grammar_inputs)

sent_processed, all_vocab, _ = sql_preprocess.basic_nltk_preproc(all_grammar_inputs, override_bigram_list=False) # ignoring bigram list

all_vocab, ind2word, word2ind = sql_preprocess.create_ind_dict(all_vocab)

VOCAB_SIZE = len(all_vocab)

sent_processed = sql_preprocess.merge_common_bigrams(sent_processed)

loaded_test = []  # store compatible length grammar based init (generation) test strings
for sent in sent_processed:
    if len(sent) <= MAX_LENGTH - 1:
        loaded_test.append(sent)

if model_variant in ['RandomFuzzer', 'GrammarGenRandMutate', 'GrammarGenAndMutate']:

    if model_variant == 'RandomFuzzer':
        rand_generation = True
        rand_mutation = True

    elif model_variant == 'GrammarGenRandMutate':
        rand_generation = False
        rand_mutation = True

    elif model_variant == 'GrammarGenAndMutate':
        rand_generation = False
        rand_mutation = False

    else:
        raise NotImplementedError(f"Model variant {model_variant} not implemented!")

    env = NaiveFuzzEnv(loaded_test=loaded_test, all_vocab=all_vocab, MAX_LENGTH=MAX_LENGTH, VOCAB_SIZE=VOCAB_SIZE, word2ind=word2ind, ind2word=ind2word, preferred_col_name=preferred_col_name,
                       steps_per_epoch=steps_per_epoch, CREATE_DB=CREATE_DB, rand_generation=rand_generation, rand_mutation=rand_mutation)

    agg_rewards = []
    all_exception_count = 0
    global_step_count = 0

    num_actions = MAX_LENGTH * VOCAB_SIZE * 2  # x2 to accommodate insertion
    print('num_actions: ', num_actions, f'{MAX_LENGTH}*{VOCAB_SIZE}*2')

    observation, episode_return, episode_length = env.reset(), 0, 0
    expt_start_time = time()
    all_success_strings = []

    for epoch in range(epochs):
        # Initialize the sum of the returns, lengths and number of episodes for each epoch
        sum_return = 0
        sum_length = 0
        num_episodes = 0

        # Iterate over the steps of each epoch
        for t in range(steps_per_epoch):

            global_step_count += 1

            if not rand_generation:
                observation = observation.reshape(1, -1)
                action = random.randint(0, num_actions-1) # randint includes the upper bound
                action_breakdown = env.breakdown_action(action)

                observation_new, fuzzing_success, exception_success, done = env.step(action)
                obs_text_list = [ind2word[i] for i in observation[0]]
                success_str = " ".join(ind2word[i] for i in observation_new)
            else:
                obs_text_list = observation
                action = None
                action_breakdown = None
                observation_new, fuzzing_success, exception_success, done = env.step(action=action)
                success_str = observation_new

            print(epoch, t, observation, " ".join(obs_text_list), action_breakdown, observation_new, success_str, fuzzing_success, exception_success, done)

            if fuzzing_success:
                episode_return += 1
                success_time = round((time() - expt_start_time), 2)

                all_success_strings.append(success_str)
                logging.info(f"SUCCESS AT STEP {global_step_count} @ {epoch} -> {t}/{steps_per_epoch}, {success_time} seconds:")
                if not rand_generation:
                    logging.info(" ".join(obs_text_list))
                logging.info(success_str)
                wandb.log({"success_step": global_step_count})
                wandb.log({"success_time": success_time})
                wandb.log({"success_str": success_str})

            if exception_success:
                all_exception_count += 1
                wandb.log({"exception_str": success_str})
                wandb.log({"exception_step": global_step_count})

            episode_length += 1

            # Update the observation
            observation = observation_new.copy()

            # Finish trajectory if reached to a terminal state
            terminal = done
            if terminal or (t == steps_per_epoch - 1):
                sum_return += episode_return
                sum_length += episode_length
                num_episodes += 1
                observation, episode_return, episode_length = env.reset(), 0, 0  # new obs

        # Print mean return and length for each epoch
        iter_reward = round((sum_return / num_episodes), 2)

        agg_rewards.append(iter_reward)
        wandb.log({"mean_rew": iter_reward})
        wandb.log({"mean_len": round((sum_length / num_episodes), 2)})
        wandb.log({"success_count": episode_return})
        print(
            f" Epoch: {epoch + 1}. Mean Rew: {(sum_return / num_episodes):.2f}. Mean Length: {(sum_length / num_episodes):.2f}. Success Count: {episode_return}."
        )

        if epoch % 100 == 0:
            mean_agg = sum(agg_rewards[-100:]) / len(agg_rewards[-100:])
            print("Mean Rew Agg over 100 epochs: ", mean_agg)
            wandb.log({"mean_agg_rew": mean_agg})
            wandb.log({"unique_success_strings": len(set(all_success_strings))})

    print("all_success_strings: ", all_success_strings)
    print("all_success_strings UNIQUE COUNT: ", len(set(all_success_strings)))
    print(f"All exception count: {all_exception_count}/{global_step_count}")

    mean_agg = sum(agg_rewards[-100:]) / len(agg_rewards[-100:])
    wandb.log({"mean_agg_rew": mean_agg})
    wandb.log({"unique_success_strings": len(set(all_success_strings))})
    wandb.log({"all_exception_count": all_exception_count})
    wandb.log({"global_step_count": global_step_count})
    wandb.log({"exception_ratio": all_exception_count/global_step_count})

elif model_variant in ['BERT', 'GRU']:
    if model_variant == 'BERT':

        @dataclass
        class Config:
            MAX_LEN = MAX_LENGTH
            BATCH_SIZE = 32
            LR = 0.001
            VOCAB_SIZE = VOCAB_SIZE
            EMBED_DIM = 128
            NUM_HEAD = 8  # used in bert model
            FF_DIM = 128  # used in bert model
            NUM_LAYERS = 1

        config = Config()

        actor_critic_model = BERTActorCriticModel(max_length=MAX_LENGTH, vocab_size=VOCAB_SIZE, config=config, loaded_test=loaded_test,
                                                    all_vocab=all_vocab, ind2word=ind2word, word2ind=word2ind, retrain=retrain_bert)

    elif model_variant == 'GRU':

        actor_critic_model = GRUActorCriticModel(max_length=MAX_LENGTH, vocab_size=VOCAB_SIZE)

    else:

        raise NotImplementedError(f"Model variant {model_variant} not implemented!")

    # Initialize the environment and get the dimensionality of the
    # observation space and the number of possible actions
    env = RLFuzzEnv(loaded_test=loaded_test, all_vocab=all_vocab, MAX_LENGTH=MAX_LENGTH, VOCAB_SIZE=VOCAB_SIZE,
                    word2ind=word2ind, ind2word=ind2word, preferred_col_name=preferred_col_name,
                    steps_per_epoch=steps_per_epoch, CREATE_DB=CREATE_DB)
    observation_dimensions = MAX_LENGTH
    num_actions = MAX_LENGTH * VOCAB_SIZE * 2  # x2 to accommodate insertion
    print('num_actions: ', num_actions, f'{MAX_LENGTH}*{VOCAB_SIZE}*2')

    # Initialize the buffer
    buffer = Buffer(observation_dimensions, steps_per_epoch)

    # Initialize the actor and the critic as keras models
    observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)
    logits = actor_critic_model.mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)
    actor = keras.Model(inputs=observation_input, outputs=logits)
    value = tf.squeeze(
        actor_critic_model.mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1
    )
    critic = keras.Model(inputs=observation_input, outputs=value)

    # Initialize the policy and the value function optimizers
    policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)
    value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)

    # Initialize the observation, episode return and episode length
    observation, episode_return, episode_length = env.reset(), 0, 0
    expt_start_time = time()
    all_success_strings = []
    agg_rewards = []
    all_exception_count = 0
    global_step_count = 0

    """
    ## Train
    """
    # Iterate over the number of epochs
    for epoch in tqdm(range(epochs), ascii=True, unit='episodes'):
        # Initialize the sum of the returns, lengths and number of episodes for each epoch
        sum_return = 0
        sum_length = 0
        sum_fuzzing_valid_succ = 0
        num_episodes = 0

        success_count = 0

        current_bandit_store = env.fuzzer.current_bandit  # to store reward for MAB model

        ppo = PPO(actor, critic, policy_optimizer, value_optimizer, num_actions, clip_ratio)

        # Iterate over the steps of each epoch
        for t in range(steps_per_epoch):

            global_step_count += 1

            # Get the logits, action, and take one step in the environment
            observation = observation.reshape(1, -1)
            logits, action = ppo.sample_action(observation)

            obs_text_list = [ind2word[i] for i in observation[0]]

            random_value = global_step_count * -0.8 / (epochs*steps_per_epoch) + 0.9 # linearly decay from 0.9 -> 0.1 -- using slope-intercept form
            random_value = round(random_value, 3)

            observation_new, reward, fuzzing_valid_succ, exception_success, done = env.step(action[0].numpy())
            if reward > 0:
                success_count += 1
                success_time = round((time() - expt_start_time), 2)
                success_str = " ".join(ind2word[i] for i in observation_new)
                all_success_strings.append(success_str)
                logging.info(f"SUCCESS AT STEP {global_step_count} @ {epoch} -> {t}/{steps_per_epoch}, {success_time} seconds:")
                logging.info(" ".join(obs_text_list))
                logging.info(success_str)
                wandb.log({"success_step": global_step_count})
                wandb.log({"success_time": success_time})
                wandb.log({"success_str": success_str})

            if exception_success:
                all_exception_count += 1
                exception_str = " ".join(ind2word[i] for i in observation_new)
                wandb.log({"exception_str": exception_str})
                wandb.log({"exception_step": global_step_count})

            if fuzzing_valid_succ:
                sum_fuzzing_valid_succ += 1
            episode_return += reward
            episode_length += 1

            # Get the value and log-probability of the action
            value_t = critic(observation)
            logprobability_t = ppo.logprobabilities(logits, action)

            # Store obs, act, rew, v_t, logp_pi_t
            buffer.store(observation, action, reward, value_t, logprobability_t)

            # Update the observation
            observation = observation_new.copy()

            # Finish trajectory if reached to a terminal state
            terminal = done
            if terminal or (t == steps_per_epoch - 1):
                last_value = 0 if done else critic(observation.reshape(1, -1))
                buffer.finish_trajectory(last_value)
                sum_return += episode_return
                sum_length += episode_length
                num_episodes += 1
                observation, episode_return, episode_length = env.reset(), 0, 0  # new obs

        # Get values from the buffer
        (
            observation_buffer,
            action_buffer,
            advantage_buffer,
            return_buffer,
            logprobability_buffer,
        ) = buffer.get()

        # Update the policy and implement early stopping using KL divergence
        for _ in range(train_policy_iterations):
            kl = ppo.train_policy(
                observation_buffer, action_buffer, logprobability_buffer, advantage_buffer
            )
            if kl > 1.5 * target_kl:
                # Early Stopping
                break

        # Update the value function
        for _ in range(train_value_iterations):
            ppo.train_value_function(observation_buffer, return_buffer)

        # Print mean return and length for each epoch
        iter_reward = round((sum_return / num_episodes), 2)
        reward_binary = 1 if sum_fuzzing_valid_succ > 0 else 0
        env.mab_agent.rewards_log.record_action(current_bandit_store, reward_binary)
        agg_rewards.append(iter_reward)
        wandb.log({"mean_rew": iter_reward})
        wandb.log({"mean_len": round((sum_length / num_episodes), 2)})
        wandb.log({"success_count": success_count})
        wandb.log({"true_success_count": sum_fuzzing_valid_succ})
        print(
            f" Epoch: {epoch + 1}. Mean Rew: {(sum_return / num_episodes):.2f}. Mean Length: {(sum_length / num_episodes):.2f}. Success Count: {success_count}. True Success Count: {sum_fuzzing_valid_succ}"
        )

        if epoch % 100 == 0:
            mean_agg = sum(agg_rewards[-100:]) / len(agg_rewards[-100:])
            print("Mean Rew Agg over 100 epochs: ", mean_agg)
            wandb.log({"mean_agg_rew": mean_agg})
            wandb.log({"unique_success_strings": len(set(all_success_strings))})

        if epoch % 500 == 0:
            actor.save(f'models/{EXPT_TAG}/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor')
            critic.save(f'models/{EXPT_TAG}/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic')

    actor.save(f'models/{EXPT_TAG}/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor')
    critic.save(f'models/{EXPT_TAG}/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic')

    print("all_success_strings: ", all_success_strings)
    print("all_success_strings UNIQUE COUNT: ", len(set(all_success_strings)))
    print(f"All exception count: {all_exception_count}/{global_step_count}")

    mean_agg = sum(agg_rewards[-100:]) / len(agg_rewards[-100:])
    wandb.log({"mean_agg_rew": mean_agg})
    wandb.log({"unique_success_strings": len(set(all_success_strings))})
    wandb.log({"all_exception_count": all_exception_count})
    wandb.log({"global_step_count": global_step_count})
    wandb.log({"exception_ratio": all_exception_count/global_step_count})

else:

    raise NotImplementedError(f"Model variant {model_variant} not implemented!")